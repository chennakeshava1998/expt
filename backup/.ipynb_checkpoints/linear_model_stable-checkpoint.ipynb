{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/guide/eager#object_based_saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 21.764\n",
      "Loss at step 000: 20.079\n",
      "Loss at step 020: 6.238\n",
      "Loss at step 040: 3.803\n",
      "Loss at step 060: 2.936\n",
      "Loss at step 080: 2.395\n",
      "Loss at step 100: 1.985\n",
      "Loss at step 120: 1.656\n",
      "Loss at step 140: 1.388\n",
      "Loss at step 160: 1.166\n",
      "Loss at step 180: 0.983\n",
      "Loss at step 200: 0.831\n",
      "Loss at step 220: 0.705\n",
      "Loss at step 240: 0.599\n",
      "Loss at step 260: 0.510\n",
      "Loss at step 280: 0.436\n",
      "Loss at step 000: 0.373\n",
      "Loss at step 020: 0.320\n",
      "Loss at step 040: 0.275\n",
      "Loss at step 060: 0.237\n",
      "Loss at step 080: 0.205\n",
      "Loss at step 100: 0.177\n",
      "Loss at step 120: 0.154\n",
      "Loss at step 140: 0.134\n",
      "Loss at step 160: 0.116\n",
      "Loss at step 180: 0.101\n",
      "Loss at step 200: 0.089\n",
      "Loss at step 220: 0.078\n",
      "Loss at step 240: 0.068\n",
      "Loss at step 260: 0.060\n",
      "Loss at step 280: 0.052\n",
      "Loss at step 000: 0.046\n",
      "Loss at step 020: 0.041\n",
      "Loss at step 040: 0.036\n",
      "Loss at step 060: 0.032\n",
      "Loss at step 080: 0.028\n",
      "Loss at step 100: 0.025\n",
      "Loss at step 120: 0.022\n",
      "Loss at step 140: 0.019\n",
      "Loss at step 160: 0.017\n",
      "Loss at step 180: 0.015\n",
      "Loss at step 200: 0.013\n",
      "Loss at step 220: 0.012\n",
      "Loss at step 240: 0.011\n",
      "Loss at step 260: 0.009\n",
      "Loss at step 280: 0.008\n",
      "Loss at step 000: 0.007\n",
      "Loss at step 020: 0.007\n",
      "Loss at step 040: 0.006\n",
      "Loss at step 060: 0.005\n",
      "Loss at step 080: 0.005\n",
      "Loss at step 100: 0.004\n",
      "Loss at step 120: 0.004\n",
      "Loss at step 140: 0.003\n",
      "Loss at step 160: 0.003\n",
      "Loss at step 180: 0.003\n",
      "Loss at step 200: 0.002\n",
      "Loss at step 220: 0.002\n",
      "Loss at step 240: 0.002\n",
      "Loss at step 260: 0.002\n",
      "Loss at step 280: 0.001\n",
      "Loss at step 000: 0.001\n",
      "Loss at step 020: 0.001\n",
      "Loss at step 040: 0.001\n",
      "Loss at step 060: 0.001\n",
      "Loss at step 080: 0.001\n",
      "Loss at step 100: 0.001\n",
      "Loss at step 120: 0.001\n",
      "Loss at step 140: 0.001\n",
      "Loss at step 160: 0.001\n",
      "Loss at step 180: 0.000\n",
      "Loss at step 200: 0.000\n",
      "Loss at step 220: 0.000\n",
      "Loss at step 240: 0.000\n",
      "Loss at step 260: 0.000\n",
      "Loss at step 280: 0.000\n",
      "Loss at step 000: 0.000\n",
      "Loss at step 020: 0.000\n",
      "Loss at step 040: 0.000\n",
      "Loss at step 060: 0.000\n",
      "Loss at step 080: 0.000\n",
      "Loss at step 100: 0.000\n",
      "Loss at step 120: 0.000\n",
      "Loss at step 140: 0.000\n",
      "Loss at step 160: 0.000\n",
      "Loss at step 180: 0.000\n",
      "Loss at step 200: 0.000\n",
      "Loss at step 220: 0.000\n",
      "Loss at step 240: 0.000\n",
      "Loss at step 260: 0.000\n",
      "Loss at step 280: 0.000\n",
      "Loss at step 000: 0.000\n",
      "Loss at step 020: 0.000\n",
      "Loss at step 040: 0.000\n",
      "Loss at step 060: 0.000\n",
      "Loss at step 080: 0.000\n",
      "Loss at step 100: 0.000\n",
      "Loss at step 120: 0.000\n",
      "Loss at step 140: 0.000\n",
      "Loss at step 160: 0.000\n",
      "Loss at step 180: 0.000\n",
      "Loss at step 200: 0.000\n",
      "Loss at step 220: 0.000\n",
      "Loss at step 240: 0.000\n",
      "Loss at step 260: 0.000\n",
      "Loss at step 280: 0.000\n",
      "Loss at step 000: 0.000\n",
      "Loss at step 020: 0.000\n",
      "Loss at step 040: 0.000\n",
      "Loss at step 060: 0.000\n",
      "Loss at step 080: 0.000\n",
      "Loss at step 100: 0.000\n",
      "Loss at step 120: 0.000\n",
      "Loss at step 140: 0.000\n",
      "Loss at step 160: 0.000\n",
      "Loss at step 180: 0.000\n",
      "Loss at step 200: 0.000\n",
      "Loss at step 220: 0.000\n",
      "Loss at step 240: 0.000\n",
      "Loss at step 260: 0.000\n",
      "Loss at step 280: 0.000\n",
      "Loss at step 000: 0.000\n",
      "Loss at step 020: 0.000\n",
      "Loss at step 040: 0.000\n",
      "Loss at step 060: 0.000\n",
      "Loss at step 080: 0.000\n",
      "Loss at step 100: 0.000\n",
      "Loss at step 120: 0.000\n",
      "Loss at step 140: 0.000\n",
      "Loss at step 160: 0.000\n",
      "Loss at step 180: 0.000\n",
      "Loss at step 200: 0.000\n",
      "Loss at step 220: 0.000\n",
      "Loss at step 240: 0.000\n",
      "Loss at step 260: 0.000\n",
      "Loss at step 280: 0.000\n",
      "Loss at step 000: 0.000\n",
      "Loss at step 020: 0.000\n",
      "Loss at step 040: 0.000\n",
      "Loss at step 060: 0.000\n",
      "Loss at step 080: 0.000\n",
      "Loss at step 100: 0.000\n",
      "Loss at step 120: 0.000\n",
      "Loss at step 140: 0.000\n",
      "Loss at step 160: 0.000\n",
      "Loss at step 180: 0.000\n",
      "Loss at step 200: 0.000\n",
      "Loss at step 220: 0.000\n",
      "Loss at step 240: 0.000\n",
      "Loss at step 260: 0.000\n",
      "Loss at step 280: 0.000\n",
      "Final loss: 0.000\n",
      "W = [[ 0.93797773  0.65172964 -1.4692758 ]\n",
      " [ 0.6379493   1.5932382   1.1446655 ]], B = [[-1.2309631   0.13512664 -2.171315  ]\n",
      " [-0.11682683  2.267558    1.1870421 ]\n",
      " [-0.13799235  0.17569822 -0.6723861 ]\n",
      " [-0.39604276  1.1069691  -0.66916454]\n",
      " [ 0.6020156   1.0809786  -0.34057006]\n",
      " [ 0.6033825   0.75618356 -1.2436237 ]\n",
      " [ 1.0295258   0.5406835  -0.39129946]\n",
      " [ 0.04536497 -0.20946859 -0.10829589]\n",
      " [-0.5439769   0.46766627 -0.9016987 ]\n",
      " [-0.8931664   0.60873723 -0.7226992 ]]\n"
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.W = tf.Variable(tf.random_normal((2, 3)), name='weight')\n",
    "    self.W2 = tf.Variable(tf.random_normal((3, 2)), name='weight2')\n",
    "    self.B = tf.Variable(tf.random_normal((10, 3)), name='bias')\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(tf.add(tf.matmul(inputs, self.W) ,self.B), self.W2)\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal((10, 2))\n",
    "noise = tf.random_normal((10, 2))\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "  error = model(inputs) - targets\n",
    "  return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "# Define:\n",
    "# 1. A model.\n",
    "# 2. Derivatives of a loss function with respect to model parameters.\n",
    "# 3. A strategy for updating the variables based on the derivatives.\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# Training loop\n",
    "for j in range(10):\n",
    "    training_inputs = training_inputs\n",
    "    training_outputs = training_outputs\n",
    "    for i in range(300):\n",
    "      grads = grad(model, training_inputs, training_outputs)\n",
    "      optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                                global_step=tf.train.get_or_create_global_step())\n",
    "      if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=577, shape=(10, 2), dtype=float32, numpy=\n",
       "array([[-1.1934915 , -0.13093072],\n",
       "       [ 5.25725   ,  3.0324697 ],\n",
       "       [ 4.3380795 , -1.1783488 ],\n",
       "       [ 1.7971243 ,  0.5664414 ],\n",
       "       [ 3.1651754 ,  0.813314  ],\n",
       "       [ 4.1797023 ,  3.2499533 ],\n",
       "       [ 2.052215  ,  2.7795227 ],\n",
       "       [ 0.6990298 ,  1.5193088 ],\n",
       "       [ 4.261947  , -3.067617  ],\n",
       "       [ 0.5441141 ,  2.6614063 ]], dtype=float32)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
