{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (etp.py, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/keshava/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3296\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7ae326ee2a23>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from TPM_from_VCS.models.etp import etp\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/keshava/Desktop/expt/backup/TPM_from_VCS/models/etp/etp.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    import numpy as np\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from TPM_from_VCS.data import toy_data_generator\n",
    "from TPM_from_VCS.models.etp import etp\n",
    "\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to plot the points\n",
    "def display_points(data1, data2):\n",
    "    data = (data1, data2)\n",
    "    groups = ('pred_coordinates', 'physical_coordinates')\n",
    "    color = (\"red\", \"green\")\n",
    "    # Create plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    for data, color, group in zip(data, color, groups):\n",
    "        x, y = data\n",
    "        ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=30, label=group)\n",
    "\n",
    "    plt.title('Matplot scatter plot')\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.W = tf.Variable(tf.random_normal((2, 5), dtype=tf.float64), name='weight', trainable=True)\n",
    "    self.B = tf.Variable(tf.random_normal((2, 1), dtype=tf.float64), name='bias', trainable=True)\n",
    "\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    ans = []\n",
    "\n",
    "    for i in range(0,inputs.shape[0],5):\n",
    "        inp = [inputs[i], inputs[i+1], inputs[i+2], inputs[i+3], inputs[i+4]]\n",
    "        ans.append(tf.add(tf.matmul(self.W, tf.cast(tf.reshape(inp, (5, -1)), dtype=tf.float64)), self.B))\n",
    "    return ans\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "  ans = etp.get_best_etp(model(inputs), targets)\n",
    "  tf.summary.histogram(\"loss\", ans)\n",
    "\n",
    "  return ans\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape(persistent=True) as tape:\n",
    "    loss_value = loss(model, inputs, tf.cast(targets, dtype=tf.float64))\n",
    "  return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "def modulate_loss(loss_value, inputs):\n",
    "    return loss_value/inputs.shape[0]\n",
    "    \n",
    "\n",
    "def temp_grad(model, inputs, targets):\n",
    "    loss_value = loss(model, inputs, tf.cast(targets, dtype=tf.float64))\n",
    "    loss_value = modulate_loss(loss_value, inputs)\n",
    "    w_grad = np.ones((2, 5), dtype=np.float64) * loss_value\n",
    "    b_grad = np.ones((2, 1), dtype=np.float64) * loss_value\n",
    "    \n",
    "    tf.summary.histogram(\"weight_gradients\", w_grad)\n",
    "    tf.summary.histogram(\"bias_gradients\", b_grad)\n",
    "    \n",
    "    return [w_grad, b_grad]\n",
    "    \n",
    "\n",
    "# Define:\n",
    "# 1. A model.\n",
    "# 2. Derivatives of a loss function with respect to model parameters.\n",
    "# 3. A strategy for updating the variables based on the derivatives.\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) # use Adam later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 10\n",
    "a = toy_data_generator.create_dataset(dataset_size)\n",
    "\n",
    "\n",
    "training_inputs = a[0][0]\n",
    "training_outputs = a[0][1]\n",
    "\n",
    "for i in range(dataset_size-1):\n",
    "    training_inputs = np.concatenate((training_inputs, a[i][0]))\n",
    "    training_outputs = np.concatenate((training_outputs, a[i][1]))\n",
    "\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "\n",
    "# display_points(model(training_inputs), training_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(training_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
